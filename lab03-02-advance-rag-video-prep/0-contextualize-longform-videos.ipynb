{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6416fcdf-75c5-49fc-a9ec-adb8018df284",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Achieve video understanding using Generative AI on AWS\n",
    "\n",
    "This workshop focuses on contextualizing long-form video assets, which are among the most challenging media types to analyze. We will systematically demonstrate how to break down lengthy videos into logical segments and then generate relevant insights for each segment using generative AI on AWS. This process enables a granular understanding of long-form video content at the scene level. By completing this workshop, you will learn our video understanding techniques that allow users not only to comprehend and quickly find specific videos but also to understand and identify relevant scenes or clips within those videos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b2fe83-faea-4cfe-b1ba-89fe5ce86335",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Workflow steps\n",
    "\n",
    "1. Setup prequisite and upload a media asset to Amazon Simple Storage Service (S3).\n",
    "2. Generate the audio chapter points: we use Amazon Transcribe, Automatic Speech Recognition (ASR) service to generate transcription from the audio dialogues of the media asset. then use Anthropic's Claude 3 Haiku model to analyze the conversation and identify chapter points based on significantly topic changes.\n",
    "3. In parallel, generate scene grid from video frames: we sample the frames from video and use Amazon Titan Multimodal Embedding model to help group frames into shots and then group shots into scenes based on visual similarity.\n",
    "4. Align scene and audio chapter: align video scenes with the audio chapters to identify un-intrusive breaks for ad insertion\n",
    "5. Generate the contextual response: We send the the scene grid, transcription, to Anthropic Claude 3 model in Amazon Bedrock to generate relevant contextual response: such as scene description, sentiment, relevant IAB or any other custom taxonomy.\n",
    "\n",
    "![workflow](static/workflow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066c0d08-07c0-4da7-adea-aa011894b5a1",
   "metadata": {},
   "source": [
    "## Pre-req\n",
    "You must run the [workshop_setup.ipynb](../lab00-setup/workshop_setup.ipynb) notebook in `lab00-setup` before starting this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd1686b-8c50-4d53-8ac5-54cf0ffd67e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.warn(\"Warning: if you did not run lab00-setup, please go back and run the lab00 notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794e0521-337b-49bc-80be-dd71924efb9c",
   "metadata": {},
   "source": [
    "## Load the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b540f29f-0dd3-41c4-8279-a3af8977ab81",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"load bucket, region, and role....\\n\")\n",
    "# bucket and parameter stored from Initial setup lab00\n",
    "%store -r bucket\n",
    "%store -r role\n",
    "%store -r region\n",
    "\n",
    "## check all 5 values are printed and do not fail\n",
    "print(bucket)\n",
    "print(role)\n",
    "print(region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a453ab94-9b8a-42fe-a0e5-2b2e742d71fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import JSON\n",
    "from IPython.display import Video\n",
    "from IPython.display import Pretty\n",
    "import json\n",
    "import boto3\n",
    "import sagemaker\n",
    "from termcolor import colored\n",
    "import time\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "from termcolor import colored\n",
    "import glob\n",
    "import os\n",
    "from functools import cmp_to_key\n",
    "from lib import transcribe_helper as trh\n",
    "from lib import s3_helper as s3h\n",
    "from lib import chapters as chpt\n",
    "from lib import util\n",
    "from lib import embeddings\n",
    "from lib import frames\n",
    "from lib import ffmpeg_helper as ffh\n",
    "from lib import bedrock_helper as brh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54db107e-04e8-4604-9286-dac677a2eed8",
   "metadata": {},
   "source": [
    "### Download the sample video, Meridian, from Netflix\n",
    "\n",
    "The open source content is available under the [Creative Commons Attribution 4.0 International Public License](https://creativecommons.org/licenses/by/4.0/legalcode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15aca4dd-42c6-4da0-94c5-263e7bd42ed7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "mp4_file = 'Netflix_Open_Content_Meridian.mp4'\n",
    "video_dir = Path(mp4_file).stem\n",
    "\n",
    "url = f\"https://dx2y1cac29mt3.cloudfront.net/mp4/netflix/{mp4_file}\"\n",
    "\n",
    "!curl {url} -o {mp4_file}\n",
    "\n",
    "Video(mp4_file, width=640, height=360)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3644e0bb-3d2f-4668-b69e-a1ad18c2eecd",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Upload the sample video to the default Amazon S3 bucket for this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fd992c-c1be-4f98-a60c-dc0d87df5f24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = s3h.upload_object(bucket, \"contextual_ad\", mp4_file) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bf4d06-855a-41e3-8fc6-716db94c024b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Generate chapter segments based on the dialog in the video\n",
    "\n",
    "Once the video is uploaded to S3, we will leverage Amazon Transcribe and a foundation model from Bedrock to automatically generate conversational chapter points. This will help us keep track of when conversation topics start and end in the video. The process begins with Amazon Transcribe converting speech to text and generating a transcription. This transcription is then downloaded and formatted into the WebVTT format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed05b011-a6e9-4d89-a122-1bd4d881c568",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Use Amazon Transcribe to convert speech to text\n",
    "\n",
    "This section uses Amazon Transcribe to convert the speech to text and generate a WebVTT output.\n",
    "\n",
    "If you are getting `AccessDeniedException`, log on to `AWS IAM Console`, find the SageMaker Execution IAM Role, and add the following managed polices:\n",
    "- AmazonTranscribeFullAccess\n",
    "- AmazonBedrockFullAccess\n",
    "\n",
    "Also check out the pricing on [Amazon Transcribe Pricing](https://aws.amazon.com/transcribe/pricing/) in us-east-1 region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f12d43a-ea2c-4e20-bda1-8265ae19b4c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f'SageMaker execution IAM Role ARN: {role}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5163f75d-93ce-4404-9c66-5edf94e2537b",
   "metadata": {},
   "source": [
    "### Probe the video to get the stream information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bc04cd-e27a-4592-8af0-b4cc2156a554",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "stream_info = ffh.probe_stream(mp4_file)\n",
    "\n",
    "JSON(stream_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33e15b6-ee3a-4884-ae87-0dd6aafdaa24",
   "metadata": {},
   "source": [
    "### Start the transcription job and wait for it to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad04daa-1a25-46ca-8822-fe9792fd111b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# start transcription job\n",
    "\n",
    "transcribe_response = trh.transcribe(bucket, \"contextual_ad\", mp4_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70fb594a-a188-425e-9f45-722fd0243b3a",
   "metadata": {},
   "source": [
    "### Examine the results from Amazon Transcribe\n",
    "\n",
    "The response from Amazon Transcribe contains a `results` dictionary with a `transcript` that contains a text-only transcript and a collection of `items` which contain each word and punctuation in the transcript along with a confidence score and timestamp for the item. The response also contains the same transcript formatted as subtitles in either WebVTT or SRT format.  Let's take a look at these outputs.  \n",
    "\n",
    "We will be using the WebVTT output for our workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741f39a8-2b3b-4782-a617-dad8f3d0752e",
   "metadata": {},
   "source": [
    "**Transcript**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa75fdf1-996f-499a-9f3a-6aee39c71062",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transcript_filename = trh.download_transcript(transcribe_response, output_dir = video_dir)\n",
    "\n",
    "JSON(filename=transcript_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f1582f-2dde-44d6-aed4-b9ce880e0557",
   "metadata": {
    "tags": []
   },
   "source": [
    "**WebVTT Subtitles**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a03f27e-2830-4db8-86dc-44a267878f9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vtt_filename = trh.download_vtt(transcribe_response, output_dir = video_dir)\n",
    "\n",
    "!head {vtt_filename}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af6277d-19bf-41b2-804e-0144f3597914",
   "metadata": {},
   "source": [
    "### Estimate the cost of the transcription job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107506fb-2992-4102-aa1e-2bbd8b57ef8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "duration_ms = stream_info['video_stream']['duration_ms']\n",
    "transcribe_cost = trh.display_transcription_cost(duration_ms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeaef380-0974-48c3-866a-82c47f366d28",
   "metadata": {},
   "source": [
    "### Use an Amazon Bedrock to generate chapters from the subtitles \n",
    "Next, the transcript is passed to the Anthropic Claude 3 Haiku model from Amazon Bedrock. The model analyzes the transcript and suggests conversational chapter points in a specific JSON format. In the prompt, we specify that each chapter should contain a start and end timestamp along with a reason describing the topic. The prompts for the Haiku model are shown below:\n",
    "\n",
    "**System prompt**\n",
    "\n",
    "```\n",
    "You are a media operation assistant who analyses movie transcripts in WebVTT \n",
    "format and suggest chapter points based on the topic changes in the conversations. \n",
    "It is important to read the entire transcripts.\n",
    "```\n",
    "\n",
    "\n",
    "**Messages**\n",
    "\n",
    "```\n",
    "[\n",
    "    {\n",
    "        'content': 'Here is the transcripts in <transcript> tag:\\n'\n",
    "                '<transcript>{transcript}\\n</transcript>\\n',\n",
    "        'role': 'user'\n",
    "    },\n",
    "    {\n",
    "        'content': 'OK. I got the transcript. What output format?',\n",
    "        'role': 'assistant'\n",
    "    },\n",
    "    {\n",
    "        'content': 'JSON format. An example of the output:\\n'\n",
    "                '{\"chapters\": [{\"start\": \"00:00:10.000\", \"end\": \"00:00:32.000\", '\n",
    "                '\"reason\": \"It appears the chapter talks about...\"}]}\\n',\n",
    "        'role': 'user'\n",
    "    },\n",
    "    {\n",
    "        'content': '{', 'role': 'assistant'\n",
    "    }\n",
    " ]\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809b1ff6-0b91-489c-8b9a-0438292f7b5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "conversation_response = brh.analyze_conversations(vtt_filename)\n",
    "\n",
    "# show the conversation cost\n",
    "conversation_cost = brh.display_conversation_cost(conversation_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990df82c-eb67-4a84-8ce7-4499cb44f326",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Let's take a look at the conversations that were generated from the transcript \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906eced6-682a-4578-8a60-a179b93d529b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "conversations = conversation_response['content'][0]['json']\n",
    "\n",
    "JSON(conversations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c93f658-0d0f-4513-ba36-44bc843d1c0b",
   "metadata": {},
   "source": [
    "### Generate \"chapter points\" \n",
    "\n",
    "To ensure the model's output accurately reflects the original transcript, the output JSON is post-processed to merge any overlapping chapter timestamps and align the chapter boundaries with the actual caption timestamps from the WebVTT file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdebcd7-3e40-42a9-88ff-9781abba916d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## parse the conversation\n",
    "conversations = conversation_response['content'][0]['json']\n",
    "\n",
    "## merge overlapped conversation timestamps\n",
    "chapters = chpt.merge_chapters(conversations['chapters'])\n",
    "\n",
    "## validate the conversation timestamps against the caption timestamps\n",
    "captions = chpt.parse_webvtt(vtt_filename)\n",
    "chapters = chpt.validate_timestamps(chapters, captions)\n",
    "\n",
    "conversations['chapters'] = chapters\n",
    "\n",
    "## save the conversations\n",
    "util.save_to_file(os.path.join(video_dir, 'conversations.json'), conversations)\n",
    "\n",
    "JSON(conversations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3536067-54b4-457c-9402-4eda7c34933f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "estimated_cost = conversation_cost['estimated_cost'] + transcribe_cost['estimated_cost']\n",
    "estimated_cost = round(estimated_cost, 4)\n",
    "\n",
    "print('\\n')\n",
    "print('Generating \"chapter points\"')\n",
    "print('========================================================================')\n",
    "print('Transcribe cost:', colored(f\"${round(transcribe_cost['estimated_cost'], 4)}\", 'green'), f\"with duration of {colored(transcribe_cost['duration'], 'green')}s\")\n",
    "print('Bedrock cost:', colored(f\"${round(conversation_cost['estimated_cost'], 4)}\", 'green'), f\"with {colored(conversation_cost['input_tokens'], 'green')} input tokens and {colored(conversation_cost['output_tokens'], 'green')} output tokens.\")\n",
    "print('-----')\n",
    "print('Estimated cost:', colored(f\"${estimated_cost}\", 'green'))\n",
    "print('========================================================================')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f6196f-74df-4941-950e-e7f030396ec7",
   "metadata": {},
   "source": [
    "## CHECKPOINT\n",
    "\n",
    "At this point, we have taken the audio part of the video file, run Amazon Transcribe to convert the speech to text, and run Amazon Bedrock (Anthropic Claude 3 Haiku) model to analyze the conversations.\n",
    "\n",
    "Let's move on to analyzing the visual part of the video."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7871c9-11aa-4ddc-bd9b-2a2ec0931e7d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Generate a scene grid from video frames\n",
    "\n",
    "In this section, we will sample the frames from the video and use Amazon Titan Multimodal Embedding (TME) model to help group frames into shots and then group shots into scenes based on visual similarity. Each frame from a scene is used to create a single composite image or \"scene grid\" that can be used as an input to Amazon Bedrock to understand the scene. \n",
    "\n",
    "In this process, we first sample one frame per second, then employ a cosine similarity logic on the adjacent frames to group frame images into shots, which represent camera shot change events. We chose one frame per second for downsampling based on past experiences, but this can be calibrated if you have high-motion, high-frame-rate videos. \n",
    "\n",
    "Even after identifying individual camera shots, there may still be too many semantically similar shots depicting the same setting. To further cluster these into distinct scenes, we need to expand our frame comparison beyond just adjacent frames. By looking at similar frames across an expanded time window, we can identify shots that are likely part of the same contiguous scene. We calculate pairwise similarity scores between all frames within a given time window. Frames with similarity scores above a certain threshold are considered part of the same scene group. This process is performed recursively across all frames in a shot. The time window size and similarity threshold are calibrated parameters that can significantly impact scene boundary detection accuracy. In our example, we found a 3-minute time window and 0.85 similarity threshold gave the best scene clustering results across our video samples.\n",
    "\n",
    "Technically, this scene grouping process is accomplished by first indexing all video frames using TME again and storing the embeddings along with their shot information and timestamps into a vector database, as illustrated in the figure below.  For this notebok, we are using a FAIS vector store to manage embedding locally, but you can use any vector store.  The implementation in Solution Guidance for Media2Cloud on AWS uses Amazon Open Search Serverless for this purpose.\n",
    "\n",
    "![scene grouping](./static/scene-grouping.png)\n",
    "\n",
    "**NOTE:** In an automated workflow, this step can be run in parallel to generating chapter points since there is no dependency between the steps.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4394d108-a4fe-4b5a-8530-7fa6ac79f970",
   "metadata": {},
   "source": [
    "### Sample frames from the video\n",
    "\n",
    "In this section, we are extracting 1 frame per second with a resolution of `392x220` from the sample video. Using `392x220` is chosen for a reason and will be discussed in \"Generating chapter level contextual information\" section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3c3d27-06d7-420d-a93c-bb05cea95223",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "jpeg_files = ffh.extract_frames(mp4_file, stream_info, (392, 220))\n",
    "\n",
    "print(f\"Frame extracted: {len(jpeg_files)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b942415-e5b5-48d5-bac2-5d5ce37c3e91",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Group frames into shots with Amazon Titan Multimodal Embedding\n",
    "\n",
    "- Generate frame embeddings with Amazon Titan Multimodal Embedding model\n",
    "- Group frames into shots with cosine similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95be9bc8-5a9d-4848-9143-aa5f8eb0396f",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Generate frame embeddings with Amazon Titan Multimodal Embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b8675d-83f9-4597-b8b0-62275e32d269",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "frame_embeddings = embeddings.batch_generate_embeddings(jpeg_files, output_dir = video_dir)\n",
    "\n",
    "frame_embeddings_cost = embeddings.display_embedding_cost(frame_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c729fc7-a7a8-472b-a4b2-c586dddf29a5",
   "metadata": {},
   "source": [
    "#### Group adjacent frames into shots with cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe134c1-a469-4d52-be67-839c45d1e45f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "frames_in_shots = frames.group_frames_to_shots(frame_embeddings)\n",
    "\n",
    "print(f\"Number of shots: {len(frames_in_shots)} from {len(frame_embeddings)} frames\")\n",
    "\n",
    "# update shot_id in frame_embeddings dict\n",
    "for idx, frames_in_shot in enumerate(frames_in_shots):\n",
    "    for frame_id in frames_in_shot['frame_ids']:\n",
    "        frame_embeddings[frame_id]['shot_id'] = idx\n",
    "\n",
    "# save to json file\n",
    "for file, data in [\n",
    "    ('frames_in_shots.json', frames_in_shots),\n",
    "    ('frame_embeddings.json', frame_embeddings)\n",
    "]:\n",
    "    output_file = os.path.join(video_dir, file)\n",
    "    util.save_to_file(output_file, data)\n",
    "\n",
    "# plot the shot images\n",
    "frames.plot_shots(video_dir, frame_embeddings, len(frames_in_shots))\n",
    "\n",
    "print('========')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33680b6a-3944-4a0c-9bf4-667807afa609",
   "metadata": {},
   "source": [
    "### Group shots into scenes using similarity search\n",
    "\n",
    "The previous step (grouping frames to shots) compares the similarity of the adjacent frames. This step compares the frames to the rest of the frame images of the entire content. This allows us to group frame images that are further apart to group the shots into scenes.\n",
    "\n",
    "We will perform a recursive similarity search against this indexed frame corpus. For each frame, we find all other frames within a 3-minute time window that have greater than 85% contextual similarity based on their vector representations. The shot information for these highly similar frames is recorded. This process iterates across all frames within each shot. Finally, we group the shot information that were mutually identified as highly similar into distinct scene groups. This allows us to segment the initially detected shot boundaries into higher-level semantic scene boundaries based on visual and temporal coherence.\n",
    "\n",
    "![shots to scenes](./static/shots-to-scenes.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bac1ade-86e6-4f4f-b3b6-d45d1a89ec01",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "## create an index\n",
    "dimension = len(frame_embeddings[0]['embedding'])\n",
    "vector_store = embeddings.create_index(dimension)\n",
    "\n",
    "## indexing all the frames\n",
    "embeddings.index_frames(vector_store, frame_embeddings)\n",
    "\n",
    "## find similar frames for each of the frames and store in the frame_embeddings\n",
    "for frame in frame_embeddings:\n",
    "    similar_frames = embeddings.search_similarity(vector_store, frame)\n",
    "    frame['similar_frames'] = similar_frames\n",
    "\n",
    "## find all similar frames that are related to the shots and store in the frames_in_shots\n",
    "for frames_in_shot in frames_in_shots:\n",
    "    similar_frames_in_shot = frames.collect_similar_frames(frame_embeddings, frames_in_shot['frame_ids'])\n",
    "    frames_in_shot['similar_frames_in_shot'] = similar_frames_in_shot\n",
    "\n",
    "    related_shots = frames.collect_related_shots(frame_embeddings, similar_frames_in_shot)\n",
    "    frames_in_shot['related_shots'] = related_shots\n",
    "\n",
    "shots_in_scenes = frames.group_shots_in_scenes(frames_in_shots)\n",
    "\n",
    "# store the scene_id to all structs\n",
    "for scene in shots_in_scenes:\n",
    "    scene_id = scene['scene_id']\n",
    "    shot_min, shot_max = scene['shot_ids']\n",
    "    # update json files\n",
    "    for shot_id in range(shot_min, shot_max + 1):\n",
    "        frames_in_shots[shot_id]['scene_id'] = scene_id\n",
    "        for frame_id in frames_in_shots[shot_id]['frame_ids']:\n",
    "            frame_embeddings[frame_id]['scene_id'] = scene_id\n",
    "\n",
    "# update the json files\n",
    "# save to json file\n",
    "for file, data in [\n",
    "    ('shots_in_scenes.json', shots_in_scenes),\n",
    "    ('frames_in_shots.json', frames_in_shots),\n",
    "    ('frame_embeddings.json', frame_embeddings)\n",
    "]:\n",
    "    output_file = os.path.join(video_dir, file)\n",
    "    util.save_to_file(output_file, data)\n",
    "\n",
    "# plot the scene images\n",
    "frames.plot_scenes(video_dir, frame_embeddings, len(shots_in_scenes))\n",
    "\n",
    "print(f\"Number of frames: {len(frame_embeddings)}\")\n",
    "print(f\"Number of shots: {len(frames_in_shots)}\")\n",
    "print(f\"Number of scenes: {len(shots_in_scenes)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca044876-3b08-4113-8ab2-448d70c0ca06",
   "metadata": {},
   "source": [
    "## 4. Align scene and chapter\n",
    "\n",
    "At this point, we have separately processed the visual and audio cues from the video. Now, we bring them together and ensure that the transcription chapters align with the scene breaks. The last thing you want is to insert an ad during an ongoing conversation or scene. To create alignment, we will iterate over each conversational chapter, represented by its start and end timestamps, and a text description summarizing the topic. For each chapter, the code identifies the relevant video scenes that overlap or fall within the chapter's timestamp range. The output of this process is a list of chapters, where each chapter contains a list of scene IDs representing the video scenes that align with the corresponding audio conversation. After the alignment process, we have combined visual and audio cues into the final chapters. The breaks we identified are what the system suggested as ideal places for ad insertion. In real-world applications, we recommend surfacing these breaks as suggestions to the operator and having a human-in-the-loop step to confirm the final breaks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800d0967-a52f-4cc4-bfdf-9c2f27db78e9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "scenes_in_chapters = frames.group_scenes_in_chapters(\n",
    "    conversations,\n",
    "    shots_in_scenes,\n",
    "    frames_in_shots\n",
    ")\n",
    "\n",
    "for scenes_in_chapter in scenes_in_chapters:\n",
    "    chapter_id = scenes_in_chapter['chapter_id']\n",
    "    scene_min, scene_max = scenes_in_chapter['scene_ids']\n",
    "\n",
    "    # update json files\n",
    "    for scene_id in range(scene_min, scene_max + 1):\n",
    "        shots_in_scenes[scene_id]['chapter_id'] = chapter_id\n",
    "        shot_min, shot_max = shots_in_scenes[scene_id]['shot_ids']\n",
    "        for shot_id in range(shot_min, shot_max + 1):\n",
    "            frames_in_shots[shot_id]['chapter_id'] = chapter_id\n",
    "            for frame_id in frames_in_shots[shot_id]['frame_ids']:\n",
    "                frame_embeddings[frame_id]['chapter_id'] = chapter_id\n",
    "\n",
    "# update the json files\n",
    "for file, data in [\n",
    "    ('scenes_in_chapters.json', scenes_in_chapters),\n",
    "    ('shots_in_scenes.json', shots_in_scenes),\n",
    "    ('frames_in_shots.json', frames_in_shots),\n",
    "    ('frame_embeddings.json', frame_embeddings),\n",
    "]:\n",
    "    output_file = os.path.join(video_dir, file)\n",
    "    util.save_to_file(output_file, data)\n",
    "\n",
    "# plot the chapter images\n",
    "frames.plot_chapters(video_dir, frame_embeddings, len(scenes_in_chapters))\n",
    "\n",
    "print('-----')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd877b8-e455-4e02-8053-d1af1b4bee5e",
   "metadata": {},
   "source": [
    "## 5. Generate chapter level contextual information \n",
    "\n",
    "The last step is to send both the visually and audio-aligned data to Claude 3 Haiku to generate contextual information for each chapter. This is an innovative approach that takes advantage of the multimodal capabilities of the Claude 3 family of models. From our testing, these models have demonstrated the ability to capture minute details from large images and follow image sequences when provided with appropriate instructions.\n",
    "\n",
    "To prepare the input for Claude3 Haiku, we first assemble video frames associated with each chapter and create a composite image grid. Through our experimentation, we have found that the optimum image grid ratio is 7 rows by 4 columns, which will assemble a **1568 x 1540 pixel image** that fits under Claude's 5 MB image file size limit while still preserving enough detail in each individual frame tile. Furthermore, you can also assemble multiple images if needed.\n",
    "\n",
    "<img src=\"static/scene-grid.png\" width=\"50%\" >\n",
    "\n",
    "Subsequently, the composite images and transcription are fed into the prompt to generate descriptions, sentiment, and other relevant information in a single query to the Claude3 Haiku model. Not only that, but we can adapt this approach to any taxonomy or custom labeling use cases without the need to train a model each time. This is where the true power of this approach lies. The final output can be presented to a human reviewer for final confirmation if needed. Here is an example of a composite image grid and the corresponding contextual output for a specific chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66c7ff7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_chapter_frames(frame_embeddings, scenes_in_chapters):\n",
    "    num_chapters = len(scenes_in_chapters)\n",
    "    chapters_frames = [{\n",
    "        'chapter_id': i,\n",
    "        'text': '',\n",
    "        'frames': [],\n",
    "    } for i in range(num_chapters)]\n",
    "\n",
    "    for frame in frame_embeddings:\n",
    "        chapter_id = frame['chapter_id']\n",
    "        file = frame['file']\n",
    "        chapters_frames[chapter_id]['frames'].append(file)\n",
    "        chapters_frames[chapter_id]['text'] = scenes_in_chapters[chapter_id]['text']\n",
    "        \n",
    "    return chapters_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f73db58-48b9-4814-998d-42a648dc0b55",
   "metadata": {},
   "source": [
    "### Create composte images and use Anthropic Claude to generate contextual information for each chapter\n",
    "\n",
    "**System Prompt:**\n",
    "```\n",
    "You are a media operation engineer. Your job is to review a portion of a video content presented in a sequence of consecutive images. Each image also contains a sequence of frames presented in a 4x7 grid reading from left to right and then from top to bottom. You may also optionally be given the conversation of the scene that helps you to understand the context. You are asked to provide the following information: a detail description to describe the scene, identify the sentiment, brands and logos that may appear in the scene, and five most relevant tags from the scene. It is important to return the results in JSON format and also includes a confidence score from 0 to 100. Skip any explanation.\n",
    "```\n",
    "\n",
    "**Prompt Body:**\n",
    "\n",
    "```json\n",
    "{'anthropic_version': 'bedrock-2023-05-31',\n",
    " 'max_tokens': 4096,\n",
    " 'messages': [{'content': [{'text': 'Here are X images containing frame '\n",
    "                                    'sequence that describes a scene.',\n",
    "                            'type': 'text'},\n",
    "                           {'source': {'data': <IMAGE_GRID>,\n",
    "                                       'media_type': 'image/jpeg',\n",
    "                                       'type': 'base64'},\n",
    "                            'type': 'image'}],\n",
    "               'role': 'user'},\n",
    "              {'content': 'Got the images. Do you have the conversation of the '\n",
    "                          'scene?',\n",
    "               'role': 'assistant'},\n",
    "              {'content': 'No conversation.', 'role': 'user'},\n",
    "              {'content': 'OK. Do you have other information to provide?',\n",
    "               'role': 'assistant'},\n",
    "              {'content': [{'text': 'Here is a list of Sentiments in '\n",
    "                                    '<sentiment> tag:\\n'\n",
    "                                    '<sentiment>\\n'\n",
    "                                    <SENTIMENT_LABELS>\n",
    "                                    '</sentiment>\\n'\n",
    "                                    'Only answer the sentiment from this list.',\n",
    "                            'type': 'text'}],\n",
    "               'role': 'user'},\n",
    "              {'content': 'OK. What output format?', 'role': 'assistant'},\n",
    "              {'content': 'Return JSON format. An example of the output:\\n'\n",
    "                          <JSON_EXAMPLE>\n",
    "               'role': 'user'},\n",
    "              {'content': '{', 'role': 'assistant'}]\n",
    "```\n",
    "\n",
    "**Output Example:**\n",
    "```json\n",
    "{\n",
    "    \"description\": {\n",
    "        \"text\": \"The scene describes...\",\n",
    "        \"score\": 98\n",
    "    }, \n",
    "    \"sentiment\": {\n",
    "        \"text\": \"Positive\", \n",
    "        \"score\": 90\n",
    "    }, \n",
    "    \"brands_and_logos\": [\n",
    "        {\"text\": \"Amazon\", \"score\": 95}, \n",
    "        {\"text\": \"Nike\", \"score\": 85}\n",
    "    ], \n",
    "    \"relevant_tags\": [\n",
    "        {\"text\": \"BMW\", \"score\": 95}\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979340b2-de52-42de-9f0a-524933f7a323",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "total_usage = {\n",
    "    'input_tokens': 0,\n",
    "    'output_tokens': 0,\n",
    "}\n",
    "\n",
    "frames_in_chapters = get_chapter_frames(frame_embeddings, scenes_in_chapters)\n",
    "\n",
    "for frames_in_chapter in frames_in_chapters:\n",
    "    chapter_id = frames_in_chapter['chapter_id']\n",
    "    text = frames_in_chapter['text']\n",
    "    ch_frames = frames_in_chapter['frames']\n",
    "\n",
    "    composite_images = frames.create_composite_images(ch_frames)\n",
    "    num_images = len(composite_images)\n",
    "\n",
    "    for j in range(num_images):\n",
    "        composite_image = composite_images[j]\n",
    "        print(f\"Chapter #{chapter_id:02d}: {j + 1} of {num_images} composite images\")\n",
    "        w, h = composite_image.size\n",
    "        scaled = composite_image.resize((w // 4, h // 4))\n",
    "        display(scaled)\n",
    "\n",
    "    contextual_response = brh.get_contextual_information(composite_images, text)\n",
    "    \n",
    "    # close the images\n",
    "    for composite_image in composite_images:\n",
    "        composite_image.close()\n",
    "\n",
    "    usage = contextual_response['usage']\n",
    "    contextual = contextual_response['content'][0]['json']\n",
    "\n",
    "    # save the contextual to the chapter\n",
    "    scenes_in_chapters[chapter_id]['contextual'] = {\n",
    "        'usage': usage,\n",
    "        **contextual\n",
    "    }\n",
    "\n",
    "    total_usage['input_tokens'] += usage['input_tokens']\n",
    "    total_usage['output_tokens'] += usage['output_tokens']\n",
    "\n",
    "    print(f\"==== Chapter #{chapter_id:02d}: Contextual information ======\")\n",
    "    for key in ['description', 'sentiment']:\n",
    "        print(f\"{key.capitalize()}: {colored(contextual[key]['text'], 'green')} ({contextual[key]['score']}%)\")\n",
    "\n",
    "    for key in ['brands_and_logos', 'relevant_tags']:\n",
    "        items = ', '.join([item['text'] for item in contextual[key]])\n",
    "        if len(items) == 0:\n",
    "            items = 'None'\n",
    "        print(f\"{key.capitalize()}: {colored(items, 'green')}\")\n",
    "    print(f\"================================================\\n\\n\")\n",
    "\n",
    "output_file = os.path.join(video_dir, 'scenes_in_chapters.json')\n",
    "util.save_to_file(output_file, scenes_in_chapters)\n",
    "\n",
    "contextual_cost = brh.display_contextual_cost(total_usage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c80aad-a5db-496b-a15a-43cb9d934fee",
   "metadata": {},
   "source": [
    "### Total estimated cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ee73b4-b408-425d-87d0-df938dd703c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "total_estimated_cost = 0\n",
    "\n",
    "for estimated_cost in [transcribe_cost, conversation_cost, frame_embeddings_cost, contextual_cost]:\n",
    "    total_estimated_cost += estimated_cost['estimated_cost']\n",
    "total_estimated_cost = round(total_estimated_cost, 4)\n",
    "\n",
    "print('\\n')\n",
    "print('\\n== Generating chapter points ===========================================\\n')\n",
    "print('Transcribe cost:', colored(f\"${round(transcribe_cost['estimated_cost'], 4)}\", 'green'), f\"with duration of {colored(transcribe_cost['duration'], 'green')}s\")\n",
    "print('Claude cost:', colored(f\"${round(conversation_cost['estimated_cost'], 4)}\", 'green'), f\"with {colored(conversation_cost['input_tokens'], 'green')} input tokens and {colored(conversation_cost['output_tokens'], 'green')} output tokens.\")\n",
    "print('\\n== Generating image embeddings =========================================\\n')\n",
    "print('Titan cost:', colored(f\"${round(frame_embeddings_cost['estimated_cost'], 4)}\", 'green'), f\"with {colored(frame_embeddings_cost['num_embeddings'], 'green')} embeddings.\")\n",
    "print('\\n== Chapter contextual information ======================================\\n')\n",
    "print('Claude cost:', colored(f\"${round(contextual_cost['estimated_cost'], 4)}\", 'green'), f\"with {colored(contextual_cost['input_tokens'], 'green')} input tokens and {colored(contextual_cost['output_tokens'], 'green')} output tokens.\")\n",
    "print('\\n========================================================================\\n')\n",
    "print('Total estimated cost:', colored(f\"${total_estimated_cost}\", 'green'))\n",
    "print('\\n========================================================================')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e714436f-dc57-42ad-b202-bf23013a2157",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.m5.8xlarge",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
