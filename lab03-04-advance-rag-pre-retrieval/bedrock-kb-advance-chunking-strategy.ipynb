{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98d7fcc3-ded5-4820-97a1-ffe1b101ca0e",
   "metadata": {},
   "source": [
    "## RAG Pre-Retrieval Optimization - Advanced chunking strategies\n",
    "\n",
    "Chunking is a technique used to break down text data into segments before embedding, with the aim of improving the efficiency of retrieval and optimizing the context window of our downstream foundation model. Bedrock's knowledge base natively supports a variety of chunking strategies to reduce your operational burden.\n",
    "\n",
    "In this lab, we will use the Amazons SEC-10k statments thats already prepared and uploaded to S3 during workshop setup to create three knowledge bases using three natively supported chunking strategies. Then compare and contrast pros and cons of each. For more details, please refer to [How content chunking and parsing works for knowledge bases](https://docs.aws.amazon.com/bedrock/latest/userguide/kb-chunking-parsing.html).\n",
    "\n",
    "| None | Fixed Chunking | Semantic Chunking | Hierarchical Chunking |\n",
    "|------|----------------|-------------------|-----------------------|\n",
    "|Each file is treated as a single chunk. This approach is useful when you want to maintain the integrity of each document or product description.| Fixed chunking is a basic strategy to split large text documents into smaller, uniform segments. It optimizes the retrieval process for Retrieve-and-Ground (RAG) systems by breaking down documents into manageable chunks. While easy to implement and understand, fixed chunking may sometimes split sentences or concepts across chunk boundaries. However, you can adjust the chunk size and overlap parameters to tune the reulsts. In general, Fixed chunking is most suitable for simple, structured documents. | Hierarchical chunking organizes your data into a hierarchical structure, allowing for more granular and efficient retrieval based on the inherent relationships within your data. When it parses the documents, the first step is to chunk them based on parent and child chunking sizes. Where parent chunks (higher level) represent larger chunks (e.g., documents or sections), and child chunks (lower level) represent smaller chunks (e.g., paragraphs or sentences). The semantic search is done on the child chunks, but parent chunks are returned during retrieval. This will result in more comprehensive context for the foundation model. Hierarchical chunking is best suited for complex documents with nested or hierarchical structures, such as technical manuals, legal documents, or academic papers with complex formatting and nested tables. | Semantic chunking is the most computation intensive because it use a embedding model to compare and combine semantic similarity of chunks. This approach preserves the information's integrity during retrieval, ensuring accurate and contextually appropriate results. By focusing on the text's meaning and context, semantic chunking significantly improves the quality of retrieval and should be used in scenarios where maintaining the semantic integrity of the text is crucial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8948ad-70b5-43b5-b4e4-5f68c4946893",
   "metadata": {},
   "source": [
    "## Pre-req\n",
    "You must run the `[workshop_setup.ipynb]`(../lab00-setup/workshop_setup.ipynb) notebook in `lab00-setup` before starting this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fe5a82-cfca-42dd-bd72-fd35d3a7611c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.warn(\"Warning: if you did not run lab00-setup, please go back and run the lab00 notebook\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9206e504-72fc-4a9b-8583-727470fc8346",
   "metadata": {},
   "source": [
    "### Load the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7db5560-a80c-46ff-9df3-4cad2d97aabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Lab parameters....\\n\")\n",
    "%store -r amzn10k_prefix\n",
    "%store -r amzn10k_s3_path\n",
    "%store -r bucket\n",
    "print(amzn10k_prefix)\n",
    "print(amzn10k_s3_path)\n",
    "print(bucket)\n",
    "\n",
    "print(\"\\nload the vector db parameters....\\n\")\n",
    "# vector parameters stored from Initial setup lab02\n",
    "%store -r vector_collection_arn\n",
    "%store -r vector_collection_id\n",
    "%store -r vector_host\n",
    "%store -r bedrock_kb_execution_role_arn\n",
    "## check all 4 values are printed and do not fail\n",
    "print(vector_collection_arn)\n",
    "print(vector_collection_id)\n",
    "print(vector_host)\n",
    "print(bedrock_kb_execution_role_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f134cd9d-ef17-4548-b8b7-d3bad15a18a9",
   "metadata": {},
   "source": [
    "### Initialize other parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d64035e-8166-49e1-bcce-1b5ceb174e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import boto3\n",
    "import sys\n",
    "\n",
    "sys.path.append('../lab00-setup')\n",
    "from knowledge_base import BedrockKnowledgeBase\n",
    "\n",
    "# auth for opensearch\n",
    "boto3_session = boto3.Session()\n",
    "region_name = boto3_session.region_name\n",
    "# try out KB using RetrieveAndGenerate API\n",
    "bedrock_agent_runtime_client = boto3.client(\"bedrock-agent-runtime\", region_name=region_name)\n",
    "model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\" # try with both claude instant as well as claude-v2. for claude v2 - \"anthropic.claude-v2\"\n",
    "model_arn = f'arn:aws:bedrock:{region_name}::foundation-model/{model_id}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec87fd1-8408-42b8-9b22-92a146a144ca",
   "metadata": {},
   "source": [
    "### Create the Knowledge Bases w/ different chunking strategy\n",
    "\n",
    "Let's create the Knowledge Bases for Amazon Bedrock to store the Amazons SEC-10k statments. Knowledge Bases allow you to integrate with different vector databases including Amazon OpenSearch Serverless, Amazon Aurora, Pinecone, Redis Enterprise and MongoDB Atlas. For this example, we will integrate the knowledge base with Amazon OpenSearch Serverless. The embedding model we use is `amazon.titan-embed-text-v2:0`.\n",
    "\n",
    "Here are the possible values for \"chunkingStrategy\" atribute: \"NONE | FIXED_SIZE | HIERARCHICAL | SEMANTIC\". NONE was used in previous Naive RAG labs. Now we are going to try the other 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af11742c-8159-4eb3-92fa-88563ed7e883",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "kb_mapping = dict()\n",
    "\n",
    "for chucking_strategy in [\"FIXED_SIZE\", \"HIERARCHICAL\", \"SEMANTIC\"]:\n",
    "\n",
    "    # create a object for each chunking strategy\n",
    "    kb_mapping[chucking_strategy] = dict()\n",
    "    \n",
    "    # Create knowledge base\n",
    "    suffix = random.randrange(200, 900)\n",
    "    kb_name = f\"bedrock-{chucking_strategy.lower().strip('_')}-{suffix}\"\n",
    "    index_name = f\"bedrock-{chucking_strategy.lower().replace('_', '')}-{suffix}\"\n",
    "    description = \"This knowledge base contain Amazon 10K financial document from 2022 and 2023\"\n",
    "    \n",
    "    knowledge_base = BedrockKnowledgeBase(\n",
    "        kb_name=kb_name,\n",
    "        kb_description=description,\n",
    "        data_bucket_name=bucket,\n",
    "        data_prefix=[amzn10k_prefix],\n",
    "        vector_collection_arn=vector_collection_arn,\n",
    "        vector_collection_id=vector_collection_id,\n",
    "        vector_host=vector_host,\n",
    "        bedrock_kb_execution_role_arn=bedrock_kb_execution_role_arn,\n",
    "        index_name=index_name,\n",
    "        suffix=suffix,\n",
    "        chunking_strategy=chucking_strategy\n",
    "    )\n",
    "\n",
    "    kb_mapping[chucking_strategy][\"KnowledgeBase\"] = knowledge_base\n",
    "    # ensure that the kb is available\n",
    "    time.sleep(30)\n",
    "    # Start the data ingestion\n",
    "    knowledge_base.start_ingestion_job()\n",
    "    kb_mapping[chucking_strategy][\"KbId\"] = knowledge_base.get_knowledge_base_id()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6a4968-8381-4d8e-999b-1cfa2d38762b",
   "metadata": {},
   "source": [
    "### Prompt to test\n",
    "\n",
    "we are going to use the same prompt and test against all the different knowledge base with different chucking strategy to compare\n",
    "\n",
    "\"What is Amazon doing in the field of entertainment, movies and cinema?\"\n",
    "\"Key challenges faced by Amazon in year 2022 and 2023\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdbb919-c09e-4b75-bb55-a178ed4732dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Key challenges faced by Amazon in year 2022 and 2023\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe5f163-de08-457b-8c7b-f55ac0550885",
   "metadata": {},
   "source": [
    "### Generate and render the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d80401-ced4-471c-8ca8-c019f7bc45fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chucking_strategy in [\"FIXED_SIZE\", \"HIERARCHICAL\", \"SEMANTIC\"]:\n",
    "\n",
    "    print(\"========================================================================================\")\n",
    "    print(f\"Generate a response using ({chucking_strategy}) chucking knowledge base\")\n",
    "    \n",
    "    response_ret = bedrock_agent_runtime_client.retrieve_and_generate(\n",
    "        input={\n",
    "            \"text\": prompt\n",
    "        },\n",
    "        retrieveAndGenerateConfiguration={\n",
    "            \"type\": \"KNOWLEDGE_BASE\",\n",
    "            \"knowledgeBaseConfiguration\": {\n",
    "                'knowledgeBaseId': kb_mapping[chucking_strategy][\"KbId\"],\n",
    "                \"modelArn\": \"arn:aws:bedrock:{}::foundation-model/{}\".format(region_name, \n",
    "                                                                             model_id),\n",
    "                \"retrievalConfiguration\": {\n",
    "                    \"vectorSearchConfiguration\": {\n",
    "                        \"numberOfResults\":3\n",
    "                    } \n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    # generated text output\n",
    "    kb_mapping[chucking_strategy][\"Response\"] = response_ret['output']['text']\n",
    "    \n",
    "    response_ret = bedrock_agent_runtime_client.retrieve(\n",
    "        knowledgeBaseId=kb_mapping[chucking_strategy][\"KbId\"],\n",
    "        retrievalQuery={\n",
    "            'text': prompt\n",
    "        },\n",
    "        retrievalConfiguration={\n",
    "            'vectorSearchConfiguration': {\n",
    "                \"numberOfResults\":3\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    \n",
    "\n",
    "    # generated text output\n",
    "    kb_mapping[chucking_strategy][\"SearchResults\"] = response_ret['retrievalResults']\n",
    "    \n",
    "    print(\"========================================================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf1f4df-5b89-4be6-a5a6-9de7b9ea65d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# First, determine the maximum length needed\n",
    "max_length = 0\n",
    "for key in kb_mapping:\n",
    "    # Count response + separator + search results\n",
    "    current_length = 2 + len(kb_mapping[key][\"SearchResults\"])  # 2 for response and separator\n",
    "    max_length = max(max_length, current_length)\n",
    "\n",
    "display_map = dict()\n",
    "        \n",
    "# reformat results\n",
    "for key in kb_mapping:\n",
    "    display_map[key] = []\n",
    "    \n",
    "    # Add response\n",
    "    response = kb_mapping[key][\"Response\"]\n",
    "    display_map[key].append(response)\n",
    "    \n",
    "    # Add separator\n",
    "    display_map[key].append(\"======[Search Results]======\")\n",
    "    \n",
    "    # Add search results\n",
    "    for result in kb_mapping[key][\"SearchResults\"]:\n",
    "        display_map[key].append(f'{result[\"content\"][\"text\"][:1000]}...')\n",
    "    \n",
    "    # Pad with empty strings if needed\n",
    "    while len(display_map[key]) < max_length:\n",
    "        display_map[key].append(\"\")\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(display_map)\n",
    "\n",
    "output = \"\"\n",
    "output += df.style.hide()._repr_html_()\n",
    "output += \"&nbsp;\"\n",
    "\n",
    "display(HTML(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70c5a26-12e3-4569-9a01-4774fb8b928d",
   "metadata": {},
   "source": [
    "Looking at these three responses comparing Amazon's challenges across different categorizations (FIXED_SIZE, HIERARCHICAL, and SEMANTIC), they all cover similar core challenges but present them slightly differently:\n",
    "\n",
    "Common Themes Across All Three:\n",
    "1. Foreign exchange rate fluctuations impact\n",
    "2. Economic conditions and geopolitical changes\n",
    "3. Supply chain constraints\n",
    "4. Labor market challenges\n",
    "5. COVID-19 pandemic effects\n",
    "6. Interest rate concerns\n",
    "\n",
    "Key Differences:\n",
    "\n",
    "FIXED_SIZE:\n",
    "- Most concise presentation\n",
    "- Focuses on specific financial impacts (e.g., 210 basis points impact on net sales)\n",
    "- Provides specific forecasts for Q1 2023 and Q1 2024\n",
    "\n",
    "HIERARCHICAL:\n",
    "- More detailed organization of challenges\n",
    "- Emphasizes operational aspects like product mix and third-party sellers\n",
    "- Includes broader strategic concerns like world events and new technologies\n",
    "\n",
    "SEMANTIC:\n",
    "- Most comprehensive coverage\n",
    "- Includes additional challenges like:\n",
    "  - Tax obligations\n",
    "  - Competition\n",
    "  - Managing growth\n",
    "  - Inventory management\n",
    "  - Payment risks\n",
    "  - Fulfillment optimization\n",
    "\n",
    "All three perspectives provide valuable insights, with SEMANTIC offering the most detailed view, HIERARCHICAL providing good structural organization, and FIXED_SIZE giving the most concise financial impact assessment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe8d3a2-1ab2-481b-b81c-d71748c23923",
   "metadata": {},
   "source": [
    "### Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0efac2-d594-48c5-8374-d66107cafa8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in kb_mapping:\n",
    "    kb_mapping[key][\"KnowledgeBase\"].delete_kb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744de791-f918-404d-9067-ae2d64b53f43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948db64a-461c-4a87-80cc-3fe98b695397",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
